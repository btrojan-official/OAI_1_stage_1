{"cells":[{"cell_type":"markdown","metadata":{"id":"te8R-UxTIRCa"},"source":["# Analiza zależnościowa\n","\n","![image.png](attachment:image.png)\n","\n","## Wstęp\n","\n","Język, którym posługujemy się na co dzień, funkcjonuje na zasadzie kompozycyjności. Oznacza to, że znaczenie złożonych wyrażeń językowych można wywnioskować z ich części składowych i z relacji między nimi. Ta właściwość pozwala użytkownikom języka na daleko idącą kreatywność w sposobie konstruowania wypowiedzi, przy zachowaniu precyzji komunikacji. Sposób w jaki słowa w zdaniu są ze sobą związane, tworzy strukturę ukorzenionego drzewa. Problemem, który rozważamy w tym zadaniu, jest automatyczna konstrukcja takich drzew dla zdań w języku polskim. Problem nosi nazwę analizy składniowej zdań, a konkretnie dokonywać będziemy analizy zależnościowej.\n","\n","Analiza składniowa jest w ogólności trudna. Na przykład, mimo że zdania `(1) Maria do jutra jest zajęta.` oraz `(2) Droga do domu jest zajęta.` zawierają kolejno te same części mowy, w dodatku o dokładnie tej samej formie gramatycznej, to w zdaniu (1) fraza \"do jutra\" modyfikuje czasownik \"jest zajęta\", natomiast w zdaniu (2) fraza \"do domu\" jest podrzędnikiem rzeczownika \"droga\". W dodatku, czasami nawet natywni użytkownicy języka mogą zinterpretować strukturę zdania na dwa różne sposoby: zdanie `Zauważyłem dziś samochód Adama, którego dawno nie widziałem.` może być interpretowane na dwa sposoby w zależności od tego, do czego odnosi się \"którego\": czy do \"samochodu Adama\", czy może do \"Adama\".\n","\n","Istnieje wiele różnych algorytmów rozwiązujących problem analizy zależnościowej. Klasyczne metody przetwarzają zdanie słowo po słowie, od lewej do prawej i wstawiają krawędzie w oparciu albo o pewien ustalony zbiór reguł lub o algorytm uczenia maszynowego. W tym zadaniu użyjemy innej metody. Twoim zadaniem będzie przewidzenie drzewa zależnościowego w oparciu o wektory słów otrzymane modelem HerBERT.\n","\n","HerBERT to polska wersja BERT, który jest modelem językowym i działa następująco:\n","1. BERT posiada moduł nazywany tokenizatorem (ang. tokenizer), który dzieli zdanie na pewne podsłowa. Na przykład zdanie `Dostaję klucz i biegnę do swojego pokoju.` dzieli na `'Dosta', 'ję', 'klucz', 'i', 'bieg', 'nę', 'do', 'swojego', 'pokoju', '.'`. Tokenizator jest wyposażony w słownik, który podsłowom przypisuje unikalne liczby: w praktyce zatem otrzymujemy mało zrozumiałe dla człowieka `18577, 2779, 22816, 1009, 4775, 2788, 2041, 5058, 7217, 1899`.\n","1. Następnie BERT posiada słownik, który zamienia te liczby na wektory o długości 768. Otrzymujemy zatem macierz o rozmiarach `10 x 768`.\n","1. BERT posiada 12 warstw, z których każda bierze wynik poprzedniej i wykonuje na niej pewną transformację. Szczegóły nie są istotne w tym zadaniu! Ważne jest natomiast to, że cały model jest uczony automatycznie, przy użyciu dużych korpusów tekstu. Zinterpretowanie działania każdej warstwy jest niemożliwe! Natomiast być może w skomplikowanym algorytmie, którego nauczył się BERT różne warstwy pełnią różne role.\n","\n","## Zadanie\n","\n","Twoim zadaniem będzie automatyczna analiza składniowa zdań w języku polskim. Pominiemy dokładne objaśnienie sposobu konstruowania takich drzew, możesz samemu popatrzeć na przykłady! Dostaniesz zbiór danych treningowych zawierający 1000 przykładów rozkładów zdań. W pliku `train.conll` znajdują się poetykietowane zdania, na przykład:\n","\n","| # | Word         | - | - | - | - | Head | - | - | - |\n","|---|--------------|---|---|---|---|--------|---|---|---|\n","| 1 | Wyobraź      | _ | _ | _ | _ | 0      | _ | _ | _ |\n","| 2 | sobie        | _ | _ | _ | _ | 1      | _ | _ | _ |\n","| 3 | człowieka    | _ | _ | _ | _ | 1      | _ | _ | _ |\n","| 4 | znajdującego | _ | _ | _ | _ | 3      | _ | _ | _ |\n","| 5 | się          | _ | _ | _ | _ | 4      | _ | _ | _ |\n","| 6 | na           | _ | _ | _ | _ | 4      | _ | _ | _ |\n","| 7 | ogromnej     | _ | _ | _ | _ | 8      | _ | _ | _ |\n","| 8 | górze        | _ | _ | _ | _ | 6      | _ | _ | _ |\n","| 9 | .            | _ | _ | _ | _ | 1      | _ | _ | _ |\n","\n","Co jest sposobem na zakodowanie następującego drzewa składniowego zdania złożonego:\n","```\n","      Wyobraź                          \n","   ______|_____________                 \n","  |      |         człowieka           \n","  |      |             |                \n","  |      |        znajdującego         \n","  |      |      _______|__________      \n","  |      |     |                  na   \n","  |      |     |                  |     \n","  |      |     |                górze  \n","  |      |     |                  |     \n","sobie    .    się              ogromnej\n","```\n","Dostarczamy Ci funkcję w Pythonie służącą do wczytania przykładów z tego pliku i na ich wizualizację. Twoje rozwiązanie powinno:\n","1. Dzielić zdanie na podsłowa.\n","1. Dla każdego podsłowa przypisywać wektor. Należy użyć tutaj finalnych lub pośrednich wektorów wyliczonych przez model HerBERT.\n","1. Agregować wektory podsłów tak aby otrzymać wektory słów.\n","1. Zaimplementować i wyuczyć prosty model przewidujący odległości w drzewie i głębokości w drzewie poszczególnych słów w zdaniu.\n","1. Użyć modeli odległości i głębokości do skonstruowania drzewa składniowego.\n","\n","\n","## Ograniczenia\n","- Twoje finalne rozwiązanie będzie testowane w środowisku **bez** GPU.\n","- Ewaluacja twojego rozwiązania (bez treningu) na 200 przykładach testowych powinna trwać nie dłużej niż 5 minut na Google Colab bez GPU.\n","- Do dyspozycji masz model typu BERT: `allegro/herbert-base-cased` oraz tokenizer `allegro/herbert-base-cased`. Nie wolno korzystać z innych uprzednio wytrenowanych modeli oraz ze zbiorów danych innych niż dostarczony.\n","- Lista dopuszczalnych bibliotek: `transformers`, `nltk`, `torch`.\n","\n","## Uwagi i wskazówki\n","- Liczne wskazówki znajdują się we wzorcach funkcji, które powinieneś zaimplementować.\n","\n","## Pliki zgłoszeniowe\n","Rozwiązanie zadania stanowi plik archiwum zip zawierające:\n","1. Ten notebook\n","2. Plik z wagami modelu odległości: `distance_model.pth`\n","3. Plik z wagami modelu głębokości: `depth_model.pth`\n","\n","Uruchomienie całego notebooka z flagą `FINAL_EVALUATION_MODE` ustawioną na `False` powinno w maksymalnie 10 minut skutkować utworzeniem obu plików z wagami.\n","\n","## Ewaluacja\n","Podczas sprawdzania flaga `FINAL_EVALUATION_MODE` zostanie ustawiona na `True`, a następnie zostanie uruchomiony cały notebook.\n","Zaimplementowana przez Ciebie funkcja `parse_sentence`, której wzorzec znajdziesz na końcu tego notatnika, zostanie oceniona na 200 przykładach testowych.\n","Ewaluacja będzie podobna do tej zaimplementowanej w funkcji `evaluate_model`.\n","Pamiętaj jednak, że ostateczna funkcja do ewaluacji sprawdzała będzie dodatkowo, czy zwracane przez twoją funkcję `parse_sentence` drzewa są poprawne!\n","\n","Ewaluacja nie może zajmować więcej niż 3 minuty. Możesz uruchomić walidację swojego rozwiązania na dostarczonym zbiorze danych walidacyjnych na Google Colab, aby przekonać się czy nie przekraczasz czasu.\n","Za pomocą skryptu `validation_script.py` będziesz mógł upewnić się, że Twoje rozwiązanie zostanie prawidłowo wykonane na naszych serwerach oceniających:\n","\n","```\n","python3 validation_script.py --train\n","python3 validation_script.py\n","```\n","\n","Podczas sprawdzania zadania, użyjemy dwóch metryk: UUAS oraz root placement.\n","1. Root placemenet oznacza ułamek przykładów na których poprawnie wskażesz korzeń drzewa składniowego,\n","2. UUAS dla konkretnego zdania to ułamek poprawnie umieszczonych krawędzi. UUAS dla zbioru to średnia wyników dla poszczególnych zdań.\n","\n","\n","Za to zadanie możesz zdobyć pomiędzy pomiędzy 0 i 2 punkty. Twój wynik za to zadanie zostanie wyliczony za pomocą funkcji:\n","```Python\n","def points(root_placement, uuas):\n","    def scale(x, lower=0.5, upper=0.85):\n","        scaled = min(max(x, lower), upper)\n","        return (scaled - lower) / (upper - lower)\n","    return (scale(root_placement) + scale(uuas))\n","```\n","Innymi słowy, twój wynik jest sumą wyników za root placement i UUAS. Wynik za daną metrykę jest 0 jeśli wartość danej metryki jest poniżej 0.5 i 1 jeśli jest powyżej 0.85. Pomiędzy tymi wartościami, wynik rośnie liniowo z wartością metryki."]},{"cell_type":"markdown","metadata":{"id":"JgDgD4DTIRCg"},"source":["# Kod startowy"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"0eDZvstxIRCi","executionInfo":{"status":"ok","timestamp":1716811791442,"user_tz":-120,"elapsed":278,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}}},"outputs":[],"source":["FINAL_EVALUATION_MODE = False  # W czasie sprawdzania twojego rozwiązania, zmienimy tą wartość na True\n","DEPTH_MODEL_PATH = 'depth_model.pth'  # Nie zmieniaj!\n","DISTANCE_MODEL_PATH = 'distance_model.pth'  # Nie zmieniaj!"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"QGga0oHtIRCk","executionInfo":{"status":"ok","timestamp":1716811803487,"user_tz":-120,"elapsed":11652,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}}},"outputs":[],"source":["from typing import List\n","\n","import numpy as np\n","import torch\n","from torch.optim import Adam\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from transformers import (AutoModel, AutoTokenizer, PreTrainedModel,\n","                          PreTrainedTokenizer)\n","from utils import (ListDataset, ParsedSentence, Sentence, merge_subword_tokens,\n","                   read_conll, uuas_score)"]},{"cell_type":"code","source":["!pip install sacremoses"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jOZW_z_PJa3M","executionInfo":{"status":"ok","timestamp":1716811817783,"user_tz":-120,"elapsed":14299,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}},"outputId":"3f4c469d-a1b3-4a88-93f2-53c26cbb0f26"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.12.25)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.4)\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"b30adrTFIRCm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716811823766,"user_tz":-120,"elapsed":5990,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}},"outputId":"c34fff66-e70d-4ebd-ba49-a7f90212a76e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n","model = AutoModel.from_pretrained(\"allegro/herbert-base-cased\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"FA2G1v5BIRCn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716811824385,"user_tz":-120,"elapsed":629,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}},"outputId":"e3f0e814-40db-4c67-b878-885a97cfd603","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Przedstawione powyżej podejścia , koncepcje i metody dają duży wybór w określeniu docelowego modelu alokacji .\n","                   dają                                                   \n","  __________________|___________________________                           \n"," |          |                                 wybór                       \n"," |          |                            _______|__________                \n"," |          i                           |                  w              \n"," |    ______|__________________         |                  |               \n"," |   |      |       |      podejścia    |              określeniu         \n"," |   |      |       |          |        |                  |               \n"," |   |      |       |    Przedstawione  |                modelu           \n"," |   |      |       |          |        |        __________|_________      \n"," .   ,  koncepcje metody    powyżej    duży docelowego            alokacji\n","\n","Wyobraź sobie człowieka znajdującego się na ogromnej górze .\n"]}],"source":["train_sentences = read_conll('train.conll')  # 1000 zdań\n","val_sentences = read_conll('valid.conll')  # 200 zdań\n","\n","print(train_sentences[2])\n","train_sentences[2].pretty_print()  # wyświetl drzewo jednego zdania\n","print(train_sentences[6])"]},{"cell_type":"markdown","metadata":{"id":"o05q-_W3IRCp"},"source":["# Twoje rozwiązanie"]},{"cell_type":"code","source":["def create_graph(node_to_children):\n","    node_to_children = node_to_children.copy()\n","\n","    graph = {}\n","\n","    for i in range(len(node_to_children)):\n","        graph[i] = node_to_children[i][:]\n","\n","    for node in graph:\n","        for child in graph[node]:\n","            if(node not in graph[child]):\n","                graph[child].append(node)\n","\n","    return graph\n","\n","\n","def dfs(graph, node, parent, distances, depth):\n","    distances[node] = depth\n","    for child in graph[node]:\n","        if(child != parent):\n","            dfs(graph, child, node, distances, depth + 1)"],"metadata":{"id":"w6myhawvlNHE","executionInfo":{"status":"ok","timestamp":1716811824386,"user_tz":-120,"elapsed":18,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"t5BvpPIjIRCp","executionInfo":{"status":"ok","timestamp":1716811824387,"user_tz":-120,"elapsed":16,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}}},"outputs":[],"source":["def get_distances(sentence: ParsedSentence):\n","    graph = create_graph(sentence.node_to_children)\n","    n = len(graph)\n","    matrix = np.zeros((n,n))\n","\n","    for node in graph:\n","        distances = {}\n","        dfs(graph, node, -1, distances, 0)\n","        for i in range(n):\n","            matrix[node][i] = distances.get(i, -1)\n","            matrix[i][node] = distances.get(i, -1)\n","\n","    return matrix"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"VeFatRPDIRCs","executionInfo":{"status":"ok","timestamp":1716811824388,"user_tz":-120,"elapsed":15,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}}},"outputs":[],"source":["def get_bert_embeddings(\n","    sentences_s: List[str],\n","    tokenizer: PreTrainedTokenizer,\n","    model: PreTrainedModel,\n","    progress_bar: bool = False,\n","):\n","    tokens = []\n","    embeddings = []\n","\n","    batch_size = 8\n","\n","    batched_sentences = [sentences_s[i:i+batch_size] for i in range(0, len(sentences_s), batch_size)]\n","\n","    for batch_sentences in batched_sentences:\n","        encoded = tokenizer.batch_encode_plus(batch_sentences, padding=True, return_tensors=\"pt\")\n","\n","        with torch.no_grad():\n","            outputs = model(**encoded, output_hidden_states=True)\n","\n","        sequence_lengths = encoded[\"attention_mask\"].sum(dim=1)\n","        trimmed_encoding = [token[1:length-1] for token, length in zip(encoded[\"input_ids\"], sequence_lengths)]\n","        trimmed_outputs = [output[1:length-1] for output, length in zip(outputs.last_hidden_state, sequence_lengths)]\n","\n","        embeddings = embeddings + trimmed_outputs\n","        tokens = tokens + trimmed_encoding\n","\n","    return tokens, embeddings"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"vykbPIePIRCu","executionInfo":{"status":"ok","timestamp":1716811824391,"user_tz":-120,"elapsed":16,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}}},"outputs":[],"source":["def agg_fn(embeddings_list):\n","    word_embedding = np.sum(np.array(embeddings_list), axis=0)\n","    #mozesz sprobowac ze średnią\n","    return torch.tensor(word_embedding)\n","\n","def get_word_embeddings(sentences: List[Sentence], tokenizer, model):\n","    \"\"\"Funkcja zwraca embeddingi słów dla listy zdań, używając modelu i tokenizatora.\"\"\"\n","\n","    word_tokens = [sentence.words for sentence in sentences]\n","    subword_tokens, subword_embeddings = get_bert_embeddings([repr(sentence) for sentence in sentences], tokenizer, model)\n","\n","    embeddings = []\n","\n","    for i in range(len(sentences)):\n","        embedding = merge_subword_tokens(\n","            word_tokens[i],\n","            subword_tokens[i],\n","            subword_embeddings[i],\n","            tokenizer,\n","            agg_fn\n","        )\n","\n","        embeddings.append(embedding)\n","\n","    return embeddings"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"-FNvi0ijIRCv","executionInfo":{"status":"ok","timestamp":1716811950129,"user_tz":-120,"elapsed":125752,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}},"collapsed":true},"outputs":[],"source":["def get_datasets(sentences: List[ParsedSentence], tokenizer, model):\n","    embeddings = get_word_embeddings(sentences, tokenizer, model) # .flatten() - by móc całe batche wrzucać w model\n","    distances = [get_distances(sent) for sent in sentences]\n","    depths = [dist[sent.root][..., None] for dist, sent in zip(distances, sentences)]\n","    dataset_dist = ListDataset(list(zip(embeddings, distances, sentences)))\n","    dataset_depth = ListDataset(list(zip(embeddings, depths, sentences)))\n","    return dataset_dist, dataset_depth\n","\n","if not FINAL_EVALUATION_MODE:\n","    trainset_dist, trainset_depth =  get_datasets(train_sentences, tokenizer, model)\n","    valset_dist, valset_depth = get_datasets(val_sentences, tokenizer, model)"]},{"cell_type":"code","source":["max(len(depth[1]) for depth in trainset_depth)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SB2y7ab598WQ","executionInfo":{"status":"ok","timestamp":1716811950131,"user_tz":-120,"elapsed":29,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}},"outputId":"8f4d5b41-7843-4db3-82c5-cd872f0a594e"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["39"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","execution_count":12,"metadata":{"id":"vAAXac0pIRCw","executionInfo":{"status":"ok","timestamp":1716811950132,"user_tz":-120,"elapsed":23,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}}},"outputs":[],"source":["def pad_arrays(sequence, pad_with=np.inf):\n","    shapes = np.array([list(seq.shape) for seq in sequence])\n","    max_lens = list(shapes.max(axis=0))\n","    padded = [np.pad(\n","                seq,\n","                tuple((0, max_lens[i] - seq.shape[i]) for i in range(seq.ndim)),\n","                'constant',\n","                constant_values=pad_with\n","            ) for seq in sequence]\n","    return torch.tensor(np.array(padded))\n","\n","def my_pad_arrays(sequence, pad_with=0):\n","    padded = np.concatenate((np.array(sequence), np.full((40 - len(sequence),len(sequence[0])), pad_with)),axis=0)\n","    return torch.tensor(np.array(padded)).float()\n","\n","\n","def collate_fn(batch):\n","    embeddings, targets, sentences = zip(*batch)\n","    padded_embeddings = pad_arrays(embeddings, pad_with=0)\n","    padded_targets = pad_arrays(targets, pad_with=0)\n","    mask = padded_targets != 0\n","    return padded_embeddings, padded_targets, mask, sentences\n","\n","\n","if not FINAL_EVALUATION_MODE:\n","    dist_trainloader = DataLoader(trainset_dist, batch_size=32, shuffle=True, collate_fn=collate_fn)\n","    dist_valloader = DataLoader(valset_dist, batch_size=32, shuffle=False, collate_fn=collate_fn)\n","\n","    depth_trainloader = DataLoader(trainset_depth, batch_size=32, shuffle=True, collate_fn=collate_fn)\n","    depth_valloader = DataLoader(valset_depth, batch_size=32, shuffle=False, collate_fn=collate_fn)\n","\n","# dist_trainloader i dist_valloader zwracają krotki (embeddings, distances, masks, sentences)\n","# depths_trainloader i depths_valloader zwracają krotki (embeddings, depths, masks, sentences)\n","# embeddings.shape: (batch_size, max_seq_len, emb_dim)\n","# distances.shape: (batch_size, max_seq_len, max_seq_len)\n","# depths.shape: (batch_size, max_seq_len, 1)"]},{"cell_type":"code","source":["def only_root_target(target):\n","    new_target = torch.zeros(target.size())\n","    roots = get_roots_from_target(target, 0)\n","    for i in range(len(new_target)):\n","        new_target[i, roots[i]] = 1\n","\n","    return new_target"],"metadata":{"id":"FJc6so8Tn6WI","executionInfo":{"status":"ok","timestamp":1716811950132,"user_tz":-120,"elapsed":21,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def get_roots_from_target(target, looking_for):\n","    return torch.argmax((target == looking_for).int(), dim=1)"],"metadata":{"id":"zv-tqmSM88AE","executionInfo":{"status":"ok","timestamp":1716811950133,"user_tz":-120,"elapsed":20,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def root_and_first_depth(target):\n","    new_target = torch.zeros(target.size())\n","    for i in range(len(target)):\n","        no_zeros_before = True\n","        for j in range(len(target[i])):\n","            if target[i,j] == 1:\n","                new_target[i,j] = 1\n","            # if target[i,j] == 0 and no_zeros_before:\n","            #     new_target[i,j] = 1\n","            #     no_zeros_before = False\n","    return remove_dots(new_target)\n"],"metadata":{"id":"4WX3i4CspHzS","executionInfo":{"status":"ok","timestamp":1716811950134,"user_tz":-120,"elapsed":20,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def remove_dots(tens: torch.tensor):\n","    reversed_tensor = torch.flip(tens, [0])\n","\n","    index = torch.argmax((reversed_tensor == 1).int())\n","    last_index = len(tens) - index - 1\n","\n","    tens[last_index] = 0\n","\n","    return tens"],"metadata":{"id":"UWyx0MLGZK_2","executionInfo":{"status":"ok","timestamp":1716811950134,"user_tz":-120,"elapsed":19,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# distance_model2 = DistanceModel()\n","# distance_model2.load_state_dict(torch.load(DISTANCE_MODEL_PATH))\n","\n","# depth_model = DepthModel()\n","# depth_model.load_state_dict(torch.load(DEPTH_MODEL_PATH))\n","\n","# distance_model2.magic_bias = torch.nn.Parameter(torch.tensor(1,dtype=torch.float), requires_grad=False)\n","# distance_model2.magic_bias = learn_magic_bias(distance_model2, depth_model, tokenizer, model, depth_trainloader)\n","\n","# torch.save(distance_model2.state_dict(), DISTANCE_MODEL_PATH)\n","# print(distance_model2.magic_bias)"],"metadata":{"collapsed":true,"id":"1EcBmFmZPJ6Z","executionInfo":{"status":"ok","timestamp":1716811950135,"user_tz":-120,"elapsed":19,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def mask_target(target, mask):\n","    batch_size = target.shape[0]\n","    n = target.shape[1]\n","    padding = torch.zeros(batch_size, n, 45 - n)\n","    masked_target = torch.cat((target, padding), dim=2)\n","    masked_mask = torch.cat((mask, padding), dim=2)\n","\n","    return masked_target.float(), masked_mask"],"metadata":{"id":"UN7VDbAhsRmP","executionInfo":{"status":"ok","timestamp":1716811950135,"user_tz":-120,"elapsed":18,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":19,"metadata":{"id":"YVltsvBWIRCx","executionInfo":{"status":"ok","timestamp":1716811950136,"user_tz":-120,"elapsed":18,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}}},"outputs":[],"source":["class DistanceModel(torch.nn.Module):\n","    def __init__(self):\n","        super(DistanceModel, self).__init__()\n","        self.embedding_dim = 768\n","        self.hidden_dim  = 256\n","        self.num_layers = 2\n","\n","        self.bilstm = torch.nn.LSTM(self.embedding_dim, self.hidden_dim, self.num_layers, bidirectional=True, batch_first=True)\n","        self.fc = torch.nn.Linear(self.hidden_dim * self.num_layers, 45)\n","\n","    def forward(self, x):\n","        x, _ = self.bilstm(x)\n","        x = self.fc(x)\n","\n","        return x.squeeze(-1)\n","\n","class DepthModel(torch.nn.Module):\n","    def __init__(self):\n","        super(DepthModel, self).__init__()\n","        self.embedding_dim = 768\n","        self.hidden_dim = 256\n","        self.num_layers = 2\n","\n","        self.bilstm = torch.nn.LSTM(self.embedding_dim, self.hidden_dim, self.num_layers, bidirectional=True, batch_first=True)\n","        self.fc = torch.nn.Linear(self.hidden_dim * self.num_layers, 1)\n","\n","\n","    def forward(self, x):\n","        x, _ = self.bilstm(x)\n","        # x = torch.relu(x)\n","        x = torch.relu(self.fc(x))\n","\n","        return x.squeeze(-1)"]},{"cell_type":"code","source":["def train_distance_model(model, dataloader, valloader, epochs, lr):\n","    model.train()\n","\n","    criterion = torch.nn.MSELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    for epoch in range(epochs):\n","        loss_val = []\n","        for (embeddings, targets, masks, sentences) in dataloader:\n","            outputs = model(embeddings)\n","\n","            targets, masks = mask_target(targets, masks)\n","            outputs = outputs * masks\n","\n","            loss = criterion(outputs, targets)\n","            loss_val.append(loss.item())\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","\n","        print(f'Epoch [{epoch+1}/{epochs}], Loss: {sum(loss_val)/len(loss_val):.4f}')\n","        loss_val = []"],"metadata":{"id":"3b3qZqRXowGn","executionInfo":{"status":"ok","timestamp":1716811950136,"user_tz":-120,"elapsed":17,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# def good_length(masks):\n","#     sum = torch.sum(masks.squeeze(),1)\n","#     avg = torch.sum(sum,0)/len(sum)\n","\n","#     return avg > 9.4"],"metadata":{"id":"u9YHkmSK_aYU","executionInfo":{"status":"ok","timestamp":1716811950136,"user_tz":-120,"elapsed":15,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","import math\n","\n","def train_model(model, dataloader, valloader, epochs, lr):\n","    model.train()\n","\n","    criterion = torch.nn.MSELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    for epoch in range(epochs):\n","        for (embeddings, targets, masks, sentences) in dataloader:\n","            if ((epoch+1) % 3 != 0):\n","                masks = masks.squeeze()\n","            else:\n","                masks = torch.zeros(masks.squeeze().shape)\n","\n","                for i in range(len(sentences)):\n","                    masks[i][sentences[i].root] = 1\n","\n","                # print(targets.squeeze()[0])\n","                # print(masks[0])\n","\n","            outputs = model(embeddings)\n","\n","            outputs = outputs * masks\n","\n","            targets = targets.squeeze().float() * masks\n","\n","            loss = criterion(outputs, targets)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","\n","        # Print the loss for the current epoch\n","        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"],"metadata":{"id":"iuukgaKEvA1i","executionInfo":{"status":"ok","timestamp":1716811950490,"user_tz":-120,"elapsed":368,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","execution_count":89,"metadata":{"id":"NxdzEfMPIRCy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4ecf7130-bf1e-4d7d-9331-da22da6b7b0f","collapsed":true,"executionInfo":{"status":"ok","timestamp":1716822924777,"user_tz":-120,"elapsed":358633,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Training depth model\n","Epoch [1/29], Loss: 2.6743\n","Epoch [2/29], Loss: 2.1054\n","Epoch [3/29], Loss: 0.0000\n","Epoch [4/29], Loss: 1.1457\n","Epoch [5/29], Loss: 0.4333\n","Epoch [6/29], Loss: 0.0009\n","Epoch [7/29], Loss: 0.2230\n","Epoch [8/29], Loss: 0.4335\n","Epoch [9/29], Loss: 0.0056\n","Epoch [10/29], Loss: 0.1873\n","Epoch [11/29], Loss: 0.1145\n","Epoch [12/29], Loss: 0.0060\n","Epoch [13/29], Loss: 0.2037\n","Epoch [14/29], Loss: 0.2164\n","Epoch [15/29], Loss: 0.0094\n","Epoch [16/29], Loss: 0.0519\n","Epoch [17/29], Loss: 0.1168\n","Epoch [18/29], Loss: 0.0017\n","Epoch [19/29], Loss: 0.1344\n","Epoch [20/29], Loss: 0.0930\n","Epoch [21/29], Loss: 0.0027\n","Epoch [22/29], Loss: 0.0928\n","Epoch [23/29], Loss: 0.0548\n","Epoch [24/29], Loss: 0.0027\n","Epoch [25/29], Loss: 0.0734\n","Epoch [26/29], Loss: 0.1515\n","Epoch [27/29], Loss: 0.0004\n","Epoch [28/29], Loss: 0.0472\n","Epoch [29/29], Loss: 0.0418\n"]}],"source":["if not FINAL_EVALUATION_MODE:\n","    print(\"Training depth model\")\n","    depth_model = DepthModel()\n","    train_model(depth_model, depth_trainloader, depth_valloader, 29, 0.0005)\n","    torch.save(depth_model.state_dict(), DEPTH_MODEL_PATH)\n","\n","    # print(\"Training distance model\")\n","    # distance_model = DistanceModel()\n","    # train_distance_model(distance_model, dist_trainloader, dist_valloader, 12, 0.001)\n","    # torch.save(distance_model.state_dict(), DISTANCE_MODEL_PATH)"]},{"cell_type":"code","source":["iypol = 0\n","\n","number_of_ones = []\n","\n","for (embeddings, targets, masks, sentences) in depth_trainloader:\n","    if iypol == 0:\n","        iypol = 1\n","        print(sentences[0])\n","        print(sentences[0].heads)\n","        print(get_distances(sentences[0]))\n","    for t in targets:\n","        number_of_ones.append(sum(t.squeeze() == 1)/len(targets))\n","print(sum(number_of_ones)/len(number_of_ones))"],"metadata":{"id":"9GO7HrkV4bhZ","executionInfo":{"status":"ok","timestamp":1716810687144,"user_tz":-120,"elapsed":620,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c8061f5c-265a-4d04-f8dd-46db61743f46"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Wszystko dzieje się za sprawą wichru . . .\n","[2, 0, 2, 2, 4, 5, 2, 7, 8]\n","[[0. 1. 2. 2. 3. 4. 2. 3. 4.]\n"," [1. 0. 1. 1. 2. 3. 1. 2. 3.]\n"," [2. 1. 0. 2. 3. 4. 2. 3. 4.]\n"," [2. 1. 2. 0. 1. 2. 2. 3. 4.]\n"," [3. 2. 3. 1. 0. 1. 3. 4. 5.]\n"," [4. 3. 4. 2. 1. 0. 4. 5. 6.]\n"," [2. 1. 2. 2. 3. 4. 0. 1. 2.]\n"," [3. 2. 3. 3. 4. 5. 1. 0. 1.]\n"," [4. 3. 4. 4. 5. 6. 2. 1. 0.]]\n","tensor(0.1183)\n"]}]},{"cell_type":"code","source":["sentence = \"Wszystko dzieje się za sprawą wichru . . .\".split(\" \")\n","\n","we = get_word_embeddings([Sentence(sentence)],tokenizer, model)[0]\n","\n","distance_model.eval()\n","opt = depth_model(torch.tensor(we))\n","output = torch.round(opt)\n","root = torch.argmin(opt[:len(opt)-1],0)\n","print(f\"Root: {root}\")\n","print(output[:len(sentence)])\n"],"metadata":{"id":"8lGDQUqEdk4r","executionInfo":{"status":"error","timestamp":1716811950492,"user_tz":-120,"elapsed":15,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}},"colab":{"base_uri":"https://localhost:8080/","height":216},"outputId":"f490abc6-40b8-4735-815d-396898397f16"},"execution_count":23,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'distance_model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-bbfb3db20064>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_word_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdistance_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdepth_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'distance_model' is not defined"]}]},{"cell_type":"code","source":["# sent = train_sentences[0]\n","# parse_sentence(sent, distance_model, depth_model, tokenizer, model).pretty_print()# .pretty_print()  # Przewidziane drzewo\n","# sent.pretty_print()\n","# print(sent)"],"metadata":{"id":"EaSZ76wo7zsh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def eval_model(model, valloader):\n","#     model.eval()\n","#     loss_sum = []\n","#     criterion = torch.nn.CrossEntropyLoss()\n","\n","#     for (embeddings, targets, masks, sentences) in valloader:\n","\n","#         outputs = model(embeddings)\n","\n","#         targets = only_root_target(targets.squeeze()).float()\n","\n","#         guesses = torch.argmax(outputs, dim=1)\n","#         roots = get_roots_from_target(targets, 1)\n","\n","#         loss = criterion(outputs, targets)\n","\n","#         loss_sum.append(loss.item())\n","\n","#     print(f\"EVALUTAION = [{sum(loss_sum)/len(loss_sum)}]\")"],"metadata":{"id":"IvkJoD7i-1if"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# eval_model(depth_model, depth_valloader)"],"metadata":{"id":"Ds7GLL0_bbkB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def parse_sentence(sent: Sentence, distance_model, depth_model, tokenizer, model) -> ParsedSentence:\n","\n","    depth_model.eval()\n","    distance_model.eval()\n","\n","    embedded = get_word_embeddings([sent],tokenizer, model)[0]\n","\n","    depths = depth_model(embedded)\n","    distance_matrix = distance_model(embedded)\n","\n","    for i in range(distance_matrix.shape[0]):\n","        for j in range(i+1,distance_matrix.shape[0]):\n","            avg = (distance_matrix[i,j] + distance_matrix[j,i])/2\n","            distance_matrix[j,i] = avg\n","            distance_matrix[i,j] = avg\n","\n","\n","\n","    root = torch.argmin(depths[:len(depths)-1], 0)\n","    # print(depths)\n","    # print(root)\n","    # print(sent)\n","\n","    heads = np.full(len(sent),root+1)\n","    heads[root] = 0\n","\n","    depths = torch.round(depths)\n","\n","    # print(torch.round(depths))\n","    # print(distance_matrix[:len(sent),:len(sent)])\n","\n","    current_depth = 2\n","    while current_depth<=torch.max(depths):\n","        for i in range(len(depths)-1):\n","            if(depths[i]==current_depth):\n","                min_dist = torch.argmin(distance_matrix[i][:len(sent)-1],0)\n","                while(depths[min_dist] != current_depth - 1):\n","                    distance_matrix[i,min_dist] = np.inf\n","                    min_dist = torch.argmin(distance_matrix[i][:len(sent)-1],0)\n","\n","                    if(distance_matrix[i,min_dist] == np.inf):\n","                        depths[i] -= 1\n","                        if current_depth > 3:\n","                            current_depth -= 2\n","                        break\n","                if (distance_matrix[i,min_dist] != np.inf):\n","                    heads[i] = min_dist+1\n","        current_depth += 1\n","\n","    heads[root] = 0\n","\n","    parsed = ParsedSentence(sent.words, list(heads))\n","\n","    return parsed\n","\n"],"metadata":{"id":"7_Nd3Si5Y32P","executionInfo":{"status":"ok","timestamp":1716817857192,"user_tz":-120,"elapsed":376,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","execution_count":27,"metadata":{"id":"cgmQu0vbIRCy","executionInfo":{"status":"error","timestamp":1716811979540,"user_tz":-120,"elapsed":288,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}},"colab":{"base_uri":"https://localhost:8080/","height":216},"outputId":"fbd0728e-9396-4362-eb32-72d7e991ff96"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'parse_sentence' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-599b280f9f41>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFINAL_EVALUATION_MODE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mparse_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# .pretty_print()  # Przewidziane drzewo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'parse_sentence' is not defined"]}],"source":["# def parse_sentence(sent: Sentence, distance_model, depth_model, tokenizer, model) -> ParsedSentence:\n","#     def parse_sentence_from_edges(word_tokens, edges, root):\n","#         adj = [[] for _ in word_tokens]\n","#         for i, j in edges:\n","#             adj[i].append(j)\n","#             adj[j].append(i)\n","#         heads = [root+1] * len(word_tokens)\n","#         visited = [False] * len(word_tokens)\n","\n","#         def helper(v, parent):\n","#             heads[v] = parent + 1\n","#             visited[v] = True\n","#             for u in adj[v]:\n","#                 if u != parent and not visited[u]:\n","#                     helper(u, v)\n","#         helper(root, -1)\n","#         return ParsedSentence(word_tokens, heads)\n","\n","#     depth_model.eval()\n","#     distance_model.eval()\n","\n","#     embedded = get_word_embeddings([sent],tokenizer, model)[0]\n","\n","#     output = depth_model(embedded)\n","#     distance_matrix = distance_model(embedded)\n","\n","#     distance_matrix = torch.round(distance_matrix)\n","\n","#     edges = []\n","#     for i in range(distance_matrix.shape[0]):\n","#         for j in range(i+1,distance_matrix.shape[0]):\n","#             if distance_matrix[i,j] == 1:\n","#                 edges.append((i,j))\n","\n","#     root = torch.argmin(output[:len(output)-1], 0)\n","\n","#     parsed = parse_sentence_from_edges(sent.words, edges, root.item())\n","\n","#     return parsed\n","\n","if not FINAL_EVALUATION_MODE:\n","    sent = train_sentences[7]\n","    parse_sentence(sent, distance_model, depth_model, tokenizer, model).pretty_print()# .pretty_print()  # Przewidziane drzewo\n","    sent.pretty_print()\n","    print(sent)"]},{"cell_type":"markdown","metadata":{"id":"i_VPSmTAIRC0"},"source":["# Ewaluacja\n","Kod bardzo podobny do poniższego będzie służył do ewaluacji rozwiązania na zdaniach testowych. Wywołując poniższe komórki możesz dowiedzieć się ile punktów zdobyłoby twoje rozwiązanie, gdybyśmy ocenili je na danych walidacyjnych. Przed wysłaniem rozwiązania upewnij się, że cały notebook wykonuje się od początku do końca bez błędów i bez ingerencji użytkownika po wykonaniu polecenia `Run All`."]},{"cell_type":"code","execution_count":75,"metadata":{"id":"IW89BUvxIRC0","executionInfo":{"status":"ok","timestamp":1716818390070,"user_tz":-120,"elapsed":293,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}}},"outputs":[],"source":["def points(root_placement, uuas):\n","    def scale(x, lower=0.5, upper=0.85):\n","        scaled = min(max(x, lower), upper)\n","        return (scaled - lower) / (upper - lower)\n","    return (scale(root_placement) + scale(uuas))\n","\n","def evaluate_model(sentences: List[ParsedSentence], distance_model, depth_model, tokenizer, model):\n","    sum_uuas = 0\n","    root_correct = 0\n","    with torch.no_grad():\n","        for sent in sentences:\n","            parsed = parse_sentence(sent, distance_model, depth_model, tokenizer, model)\n","            root_correct += int(parsed.root == sent.root)\n","            sum_uuas += uuas_score(sent, parsed)\n","            # if uuas_score(sent,parsed) < 0.75:\n","            #     print(f\"Root: {sent.root}, Prediciton: {parsed.root}\")\n","            #     print(sent)\n","            #     print(\"///////////////\\n///////////////////\\n///////////////\")\n","\n","            #     sent.pretty_print()\n","            #     parsed.pretty_print()\n","\n","            #     print(get_distances(sent)[sent.root] - get_distances(parsed)[parsed.root])\n","\n","    root_placement = root_correct / len(sentences)\n","    uuas = sum_uuas / len(sentences)\n","\n","    print(f\"UUAS: {uuas * 100:.3}%\")\n","    print(f\"Root placement: {root_placement * 100:.3}%\")\n","    print(f\"Your score: {points(root_placement, uuas):.3}/2.0\")"]},{"cell_type":"code","execution_count":90,"metadata":{"id":"WfDBN4y1IRC0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716822949275,"user_tz":-120,"elapsed":24510,"user":{"displayName":"lewy 700","userId":"06061034898661850654"}},"outputId":"67381ef4-a5d8-4e4a-a4b7-7a4ee7a36042"},"outputs":[{"output_type":"stream","name":"stdout","text":["UUAS: 72.8%\n","Root placement: 85.5%\n","Your score: 1.65/2.0\n"]}],"source":["if not FINAL_EVALUATION_MODE:\n","    distance_model_loaded = DistanceModel()\n","    distance_model_loaded.load_state_dict(torch.load(DISTANCE_MODEL_PATH))\n","\n","    depth_model_loaded = DepthModel()\n","    depth_model_loaded.load_state_dict(torch.load(DEPTH_MODEL_PATH))\n","\n","    evaluate_model(val_sentences, distance_model_loaded, depth_model_loaded, tokenizer, model)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}