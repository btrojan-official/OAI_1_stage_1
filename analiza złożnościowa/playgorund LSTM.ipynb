{"cells":[{"cell_type":"markdown","metadata":{"id":"te8R-UxTIRCa"},"source":["# Analiza zależnościowa\n","\n","![image.png](attachment:image.png)\n","\n","## Wstęp\n","\n","Język, którym posługujemy się na co dzień, funkcjonuje na zasadzie kompozycyjności. Oznacza to, że znaczenie złożonych wyrażeń językowych można wywnioskować z ich części składowych i z relacji między nimi. Ta właściwość pozwala użytkownikom języka na daleko idącą kreatywność w sposobie konstruowania wypowiedzi, przy zachowaniu precyzji komunikacji. Sposób w jaki słowa w zdaniu są ze sobą związane, tworzy strukturę ukorzenionego drzewa. Problemem, który rozważamy w tym zadaniu, jest automatyczna konstrukcja takich drzew dla zdań w języku polskim. Problem nosi nazwę analizy składniowej zdań, a konkretnie dokonywać będziemy analizy zależnościowej.\n","\n","Analiza składniowa jest w ogólności trudna. Na przykład, mimo że zdania `(1) Maria do jutra jest zajęta.` oraz `(2) Droga do domu jest zajęta.` zawierają kolejno te same części mowy, w dodatku o dokładnie tej samej formie gramatycznej, to w zdaniu (1) fraza \"do jutra\" modyfikuje czasownik \"jest zajęta\", natomiast w zdaniu (2) fraza \"do domu\" jest podrzędnikiem rzeczownika \"droga\". W dodatku, czasami nawet natywni użytkownicy języka mogą zinterpretować strukturę zdania na dwa różne sposoby: zdanie `Zauważyłem dziś samochód Adama, którego dawno nie widziałem.` może być interpretowane na dwa sposoby w zależności od tego, do czego odnosi się \"którego\": czy do \"samochodu Adama\", czy może do \"Adama\".\n","\n","Istnieje wiele różnych algorytmów rozwiązujących problem analizy zależnościowej. Klasyczne metody przetwarzają zdanie słowo po słowie, od lewej do prawej i wstawiają krawędzie w oparciu albo o pewien ustalony zbiór reguł lub o algorytm uczenia maszynowego. W tym zadaniu użyjemy innej metody. Twoim zadaniem będzie przewidzenie drzewa zależnościowego w oparciu o wektory słów otrzymane modelem HerBERT.\n","\n","HerBERT to polska wersja BERT, który jest modelem językowym i działa następująco:\n","1. BERT posiada moduł nazywany tokenizatorem (ang. tokenizer), który dzieli zdanie na pewne podsłowa. Na przykład zdanie `Dostaję klucz i biegnę do swojego pokoju.` dzieli na `'Dosta', 'ję', 'klucz', 'i', 'bieg', 'nę', 'do', 'swojego', 'pokoju', '.'`. Tokenizator jest wyposażony w słownik, który podsłowom przypisuje unikalne liczby: w praktyce zatem otrzymujemy mało zrozumiałe dla człowieka `18577, 2779, 22816, 1009, 4775, 2788, 2041, 5058, 7217, 1899`.\n","1. Następnie BERT posiada słownik, który zamienia te liczby na wektory o długości 768. Otrzymujemy zatem macierz o rozmiarach `10 x 768`.\n","1. BERT posiada 12 warstw, z których każda bierze wynik poprzedniej i wykonuje na niej pewną transformację. Szczegóły nie są istotne w tym zadaniu! Ważne jest natomiast to, że cały model jest uczony automatycznie, przy użyciu dużych korpusów tekstu. Zinterpretowanie działania każdej warstwy jest niemożliwe! Natomiast być może w skomplikowanym algorytmie, którego nauczył się BERT różne warstwy pełnią różne role.\n","\n","## Zadanie\n","\n","Twoim zadaniem będzie automatyczna analiza składniowa zdań w języku polskim. Pominiemy dokładne objaśnienie sposobu konstruowania takich drzew, możesz samemu popatrzeć na przykłady! Dostaniesz zbiór danych treningowych zawierający 1000 przykładów rozkładów zdań. W pliku `train.conll` znajdują się poetykietowane zdania, na przykład:\n","\n","| # | Word      | - | - | - | - | Head | - | - | - |\n","|---|-----------|---|---|---|---|--------|---|---|---|\n","| 1 | Wyobraź   | _ | _ | _ | _ | 0      | _ | _ | _ |\n","| 2 | sobie     | _ | _ | _ | _ | 1      | _ | _ | _ |\n","| 3 | człowieka | _ | _ | _ | _ | 1      | _ | _ | _ |\n","| 4 | znajdującego | _ | _ | _ | _ | 3    | _ | _ | _ |\n","| 5 | się       | _ | _ | _ | _ | 4      | _ | _ | _ |\n","| 6 | na        | _ | _ | _ | _ | 4      | _ | _ | _ |\n","| 7 | ogromnej  | _ | _ | _ | _ | 8      | _ | _ | _ |\n","| 8 | górze     | _ | _ | _ | _ | 6      | _ | _ | _ |\n","| 9 | .         | _ | _ | _ | _ | 1      | _ | _ | _ |\n","\n","Co jest sposobem na zakodowanie następującego drzewa składniowego zdania złożonego:\n","```\n","      Wyobraź                          \n","   ______|_____________                 \n","  |      |         człowieka           \n","  |      |             |                \n","  |      |        znajdującego         \n","  |      |      _______|__________      \n","  |      |     |                  na   \n","  |      |     |                  |     \n","  |      |     |                górze  \n","  |      |     |                  |     \n","sobie    .    się              ogromnej\n","```\n","Dostarczamy Ci funkcję w Pythonie służącą do wczytania przykładów z tego pliku i na ich wizualizację. Twoje rozwiązanie powinno:\n","1. Dzielić zdanie na podsłowa.\n","1. Dla każdego podsłowa przypisywać wektor. Należy użyć tutaj finalnych lub pośrednich wektorów wyliczonych przez model HerBERT.\n","1. Agregować wektory podsłów tak aby otrzymać wektory słów.\n","1. Zaimplementować i wyuczyć prosty model przewidujący odległości w drzewie i głębokości w drzewie poszczególnych słów w zdaniu.\n","1. Użyć modeli odległości i głębokości do skonstruowania drzewa składniowego.\n","\n","\n","## Ograniczenia\n","- Twoje finalne rozwiązanie będzie testowane w środowisku **bez** GPU.\n","- Ewaluacja twojego rozwiązania (bez treningu) na 200 przykładach testowych powinna trwać nie dłużej niż 5 minut na Google Colab bez GPU.\n","- Do dyspozycji masz model typu BERT: `allegro/herbert-base-cased` oraz tokenizer `allegro/herbert-base-cased`. Nie wolno korzystać z innych uprzednio wytrenowanych modeli oraz ze zbiorów danych innych niż dostarczony.\n","- Lista dopuszczalnych bibliotek: `transformers`, `nltk`, `torch`.\n","\n","## Uwagi i wskazówki\n","- Liczne wskazówki znajdują się we wzorcach funkcji, które powinieneś zaimplementować.\n","\n","## Pliki zgłoszeniowe\n","Rozwiązanie zadania stanowi plik archiwum zip zawierające:\n","1. Ten notebook\n","2. Plik z wagami modelu odległości: `distance_model.pth`\n","3. Plik z wagami modelu głębokości: `depth_model.pth`\n","\n","Uruchomienie całego notebooka z flagą `FINAL_EVALUATION_MODE` ustawioną na `False` powinno w maksymalnie 10 minut skutkować utworzeniem obu plików z wagami.\n","\n","## Ewaluacja\n","Podczas sprawdzania flaga `FINAL_EVALUATION_MODE` zostanie ustawiona na `True`, a następnie zostanie uruchomiony cały notebook.\n","Zaimplementowana przez Ciebie funkcja `parse_sentence`, której wzorzec znajdziesz na końcu tego notatnika, zostanie oceniona na 200 przykładach testowych.\n","Ewaluacja będzie podobna do tej zaimplementowanej w funkcji `evaluate_model`.\n","Pamiętaj jednak, że ostateczna funkcja do ewaluacji sprawdzała będzie dodatkowo, czy zwracane przez twoją funkcję `parse_sentence` drzewa są poprawne!\n","\n","Ewaluacja nie może zajmować więcej niż 3 minuty. Możesz uruchomić walidację swojego rozwiązania na dostarczonym zbiorze danych walidacyjnych na Google Colab, aby przekonać się czy nie przekraczasz czasu.\n","Za pomocą skryptu `validation_script.py` będziesz mógł upewnić się, że Twoje rozwiązanie zostanie prawidłowo wykonane na naszych serwerach oceniających:\n","\n","```\n","python3 validation_script.py --train\n","python3 validation_script.py\n","```\n","\n","Podczas sprawdzania zadania, użyjemy dwóch metryk: UUAS oraz root placement.\n","1. Root placemenet oznacza ułamek przykładów na których poprawnie wskażesz korzeń drzewa składniowego,\n","2. UUAS dla konkretnego zdania to ułamek poprawnie umieszczonych krawędzi. UUAS dla zbioru to średnia wyników dla poszczególnych zdań.\n","\n","\n","Za to zadanie możesz zdobyć pomiędzy pomiędzy 0 i 2 punkty. Twój wynik za to zadanie zostanie wyliczony za pomocą funkcji:\n","```Python\n","def points(root_placement, uuas):\n","    def scale(x, lower=0.5, upper=0.85):\n","        scaled = min(max(x, lower), upper)\n","        return (scaled - lower) / (upper - lower)\n","    return (scale(root_placement) + scale(uuas))\n","```\n","Innymi słowy, twój wynik jest sumą wyników za root placement i UUAS. Wynik za daną metrykę jest 0 jeśli wartość danej metryki jest poniżej 0.5 i 1 jeśli jest powyżej 0.85. Pomiędzy tymi wartościami, wynik rośnie liniowo z wartością metryki."]},{"cell_type":"markdown","metadata":{"id":"JgDgD4DTIRCg"},"source":["# Kod startowy"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1715666248678,"user":{"displayName":"lewy 700","userId":"06061034898661850654"},"user_tz":-120},"id":"0eDZvstxIRCi"},"outputs":[],"source":["FINAL_EVALUATION_MODE = False  # W czasie sprawdzania twojego rozwiązania, zmienimy tą wartość na True\n","DEPTH_MODEL_PATH = 'depth_model.pth'  # Nie zmieniaj!\n","DISTANCE_MODEL_PATH = 'distance_model.pth'  # Nie zmieniaj!"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":376},"executionInfo":{"elapsed":13110,"status":"error","timestamp":1715666262087,"user":{"displayName":"lewy 700","userId":"06061034898661850654"},"user_tz":-120},"id":"QGga0oHtIRCk","outputId":"d7a83f50-dadf-47bf-ae70-27e014d4b413"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'utils'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-2-f8bc9343c212\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 10\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m from transformers import (AutoModel, AutoTokenizer, PreTrainedModel,\n\u001b[1;32m      9\u001b[0m                           PreTrainedTokenizer)\n\u001b[0;32m---\u003e 10\u001b[0;31m from utils import (ListDataset, ParsedSentence, Sentence, merge_subword_tokens,\n\u001b[0m\u001b[1;32m     11\u001b[0m                    read_conll, uuas_score)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["from typing import List\n","\n","import numpy as np\n","import torch\n","from torch.optim import Adam\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from transformers import (AutoModel, AutoTokenizer, PreTrainedModel,\n","                          PreTrainedTokenizer)\n","from utils import (ListDataset, ParsedSentence, Sentence, merge_subword_tokens,\n","                   read_conll, uuas_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1715666262088,"user":{"displayName":"lewy 700","userId":"06061034898661850654"},"user_tz":-120},"id":"jOZW_z_PJa3M"},"outputs":[],"source":["!pip install sacremoses"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1715666262089,"user":{"displayName":"lewy 700","userId":"06061034898661850654"},"user_tz":-120},"id":"b30adrTFIRCm"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3eacc305c0b2410a9028aad51bdfa3ce","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/229 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ebe30109f2b94fcbaa2b4d641e3f2f78","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/472 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"acc5952694574ad599e0c853f4823f1a","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/907k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5d63af95a79b44fc92cc956347ea07e4","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/556k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"07ec9290db1d4f17a842d3a536af880e","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/129 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f6410989d9c456f88d86185b28f5c79","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/654M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n","model = AutoModel.from_pretrained(\"allegro/herbert-base-cased\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1715666262089,"user":{"displayName":"lewy 700","userId":"06061034898661850654"},"user_tz":-120},"id":"FA2G1v5BIRCn"},"outputs":[{"name":"stdout","output_type":"stream","text":["Przedstawione powyżej podejścia , koncepcje i metody dają duży wybór w określeniu docelowego modelu alokacji .\n","                   dają                                                   \n","  __________________|___________________________                           \n"," |          |                                 wybór                       \n"," |          |                            _______|__________                \n"," |          i                           |                  w              \n"," |    ______|__________________         |                  |               \n"," |   |      |       |      podejścia    |              określeniu         \n"," |   |      |       |          |        |                  |               \n"," |   |      |       |    Przedstawione  |                modelu           \n"," |   |      |       |          |        |        __________|_________      \n"," .   ,  koncepcje metody    powyżej    duży docelowego            alokacji\n","\n","Wyobraź sobie człowieka znajdującego się na ogromnej górze .\n"]}],"source":["train_sentences = read_conll('train.conll')  # 1000 zdań\n","val_sentences = read_conll('valid.conll')  # 200 zdań\n","\n","print(train_sentences[2])\n","train_sentences[2].pretty_print()  # wyświetl drzewo jednego zdania\n","print(train_sentences[6])"]},{"cell_type":"markdown","metadata":{"id":"o05q-_W3IRCp"},"source":["# Twoje rozwiązanie"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1715666262089,"user":{"displayName":"lewy 700","userId":"06061034898661850654"},"user_tz":-120},"id":"w6myhawvlNHE"},"outputs":[],"source":["def create_graph(node_to_children):\n","    node_to_children = node_to_children.copy()\n","\n","    graph = {}\n","\n","    for i in range(len(node_to_children)):\n","        graph[i] = node_to_children[i][:]\n","\n","    for node in graph:\n","        for child in graph[node]:\n","            if(node not in graph[child]):\n","                graph[child].append(node)\n","\n","    return graph\n","\n","\n","def dfs(graph, node, parent, distances, depth):\n","    distances[node] = depth\n","    for child in graph[node]:\n","        if(child != parent):\n","            dfs(graph, child, node, distances, depth + 1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1715666262089,"user":{"displayName":"lewy 700","userId":"06061034898661850654"},"user_tz":-120},"id":"t5BvpPIjIRCp"},"outputs":[],"source":["def get_distances(sentence: ParsedSentence):\n","    graph = create_graph(sentence.node_to_children)\n","    n = len(graph)\n","    matrix = np.zeros((n,n))\n","\n","    for node in graph:\n","        distances = {}\n","        dfs(graph, node, -1, distances, 0)\n","        for i in range(n):\n","            matrix[node][i] = distances.get(i, -1)\n","            matrix[i][node] = distances.get(i, -1)\n","\n","    return matrix\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1715666262090,"user":{"displayName":"lewy 700","userId":"06061034898661850654"},"user_tz":-120},"id":"VeFatRPDIRCs"},"outputs":[],"source":["def get_bert_embeddings(\n","    sentences_s: List[str],\n","    tokenizer: PreTrainedTokenizer,\n","    model: PreTrainedModel,\n","    progress_bar: bool = False,\n","):\n","    tokens = []\n","    embeddings = []\n","\n","    batch_size = 8\n","\n","    batched_sentences = [sentences_s[i:i+batch_size] for i in range(0, len(sentences_s), batch_size)]\n","\n","    for batch_sentences in batched_sentences:\n","        encoded = tokenizer.batch_encode_plus(batch_sentences, padding=True, return_tensors=\"pt\")\n","\n","        with torch.no_grad():\n","            outputs = model(**encoded, output_hidden_states=True)\n","\n","        sequence_lengths = encoded[\"attention_mask\"].sum(dim=1)\n","        trimmed_encoding = [token[1:length-1] for token, length in zip(encoded[\"input_ids\"], sequence_lengths)]\n","        trimmed_outputs = [output[1:length-1] for output, length in zip(outputs.last_hidden_state, sequence_lengths)]\n","\n","        embeddings = embeddings + trimmed_outputs\n","        tokens = tokens + trimmed_encoding\n","\n","    return tokens, embeddings\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"elapsed":316,"status":"error","timestamp":1715666267909,"user":{"displayName":"lewy 700","userId":"06061034898661850654"},"user_tz":-120},"id":"vykbPIePIRCu"},"outputs":[],"source":["def agg_fn(embeddings_list):\n","    word_embedding = np.sum(np.array(embeddings_list), axis=0)\n","    #mozesz sprobowac ze średnią\n","    return torch.tensor(word_embedding)\n","\n","def get_word_embeddings(sentences: List[Sentence], tokenizer, model):\n","    \"\"\"Funkcja zwraca embeddingi słów dla listy zdań, używając modelu i tokenizatora.\"\"\"\n","\n","    word_tokens = [sentence.words for sentence in sentences]\n","    subword_tokens, subword_embeddings = get_bert_embeddings([repr(sentence) for sentence in sentences], tokenizer, model)\n","\n","    embeddings = []\n","\n","    for i in range(len(sentences)):\n","        embedding = merge_subword_tokens(\n","            word_tokens[i],\n","            subword_tokens[i],\n","            subword_embeddings[i],\n","            tokenizer,\n","            agg_fn\n","        )\n","\n","        embeddings.append(embedding)\n","\n","    return embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-FNvi0ijIRCv"},"outputs":[],"source":["def get_datasets(sentences: List[ParsedSentence], tokenizer, model):\n","    embeddings = get_word_embeddings(sentences, tokenizer, model) # .flatten() - by móc całe batche wrzucać w model\n","    distances = [get_distances(sent) for sent in sentences]\n","    depths = [dist[sent.root][..., None] for dist, sent in zip(distances, sentences)]\n","    dataset_dist = ListDataset(list(zip(embeddings, distances, sentences)))\n","    dataset_depth = ListDataset(list(zip(embeddings, depths, sentences)))\n","    return dataset_dist, dataset_depth\n","\n","\n","if not FINAL_EVALUATION_MODE:\n","    trainset_dist, trainset_depth =  get_datasets(train_sentences, tokenizer, model)\n","    valset_dist, valset_depth = get_datasets(val_sentences, tokenizer, model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SB2y7ab598WQ"},"outputs":[{"data":{"text/plain":["39"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["max(len(depth[1]) for depth in trainset_depth)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vAAXac0pIRCw"},"outputs":[],"source":["def pad_arrays(sequence, pad_with=0):\n","    shapes = np.array([list(seq.shape) for seq in sequence])\n","    max_lens = list(shapes.max(axis=0))\n","    max_lens[0] = 40\n","    padded = [np.pad(\n","                seq,\n","                tuple((0, max_lens[i] - seq.shape[i]) for i in range(seq.ndim)),\n","                'constant',\n","                constant_values=pad_with\n","            ) for seq in sequence]\n","    return torch.tensor(np.array(padded))\n","\n","def my_pad_arrays(sequence, pad_with=0):\n","    padded = np.concatenate((np.array(sequence), np.full((40 - len(sequence),len(sequence[0])), pad_with)),axis=0)\n","    return torch.tensor(np.array(padded)).float()\n","\n","\n","def collate_fn(batch):\n","    embeddings, targets, sentences = zip(*batch)\n","    padded_embeddings = pad_arrays(embeddings, pad_with=0)\n","    padded_targets = pad_arrays(targets, pad_with=0)\n","    mask = padded_targets != 0\n","    return padded_embeddings, padded_targets, mask, sentences\n","\n","\n","if not FINAL_EVALUATION_MODE:\n","    dist_trainloader = DataLoader(trainset_dist, batch_size=32, shuffle=True, collate_fn=collate_fn)\n","    dist_valloader = DataLoader(valset_dist, batch_size=32, shuffle=False, collate_fn=collate_fn)\n","\n","    depth_trainloader = DataLoader(trainset_depth, batch_size=32, shuffle=True, collate_fn=collate_fn)\n","    depth_valloader = DataLoader(valset_depth, batch_size=32, shuffle=False, collate_fn=collate_fn)\n","\n","# dist_trainloader i dist_valloader zwracają krotki (embeddings, distances, masks, sentences)\n","# depths_trainloader i depths_valloader zwracają krotki (embeddings, depths, masks, sentences)\n","# embeddings.shape: (batch_size, max_seq_len, emb_dim)\n","# distances.shape: (batch_size, max_seq_len, max_seq_len)\n","# depths.shape: (batch_size, max_seq_len, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YVltsvBWIRCx"},"outputs":[],"source":["# class DistanceModel(torch.nn.Module):\n","#     def __init__(self):\n","#         # TODO: implement me\n","#         ...\n","\n","#     def forward(self, x):\n","#         # TODO: implement me\n","#         ...\n","\n","\n","class DepthModel(torch.nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, num_layers):\n","        super(DepthModel, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","\n","        self.bilstm = torch.nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=True, batch_first=True)\n","        self.fc = torch.nn.Linear(hidden_dim * num_layers, 1)\n","\n","        self.relu = torch.nn.ReLU()\n","\n","    def forward(self, x):\n","        x, _ = self.bilstm(x)\n","        x = self.relu(self.fc(self.relu(x)))\n","        return x.squeeze(-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FJc6so8Tn6WI"},"outputs":[],"source":["def only_root_target(target: torch.tensor):\n","    new_target = torch.zeros(target.size())\n","    roots = get_roots_from_target(target)\n","    for i in range(len(new_target)):\n","        new_target[i, roots[i]] = 20\n","\n","    return new_target#, roots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zv-tqmSM88AE"},"outputs":[],"source":["def get_roots_from_target(target):\n","    return torch.argmax((target == 0).int(), dim=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QzkB1rLp-0o1"},"outputs":[],"source":["# Hyperparameters\n","EMBEDDING_DIM = 768\n","HIDDEN_DIM = 256\n","NUM_LAYERS = 2\n","BATCH_SIZE = 32\n","MAX_SEQ_LEN = 40"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"NxdzEfMPIRCy"},"outputs":[],"source":["import torch.nn.functional as F\n","import math\n","\n","def loss_fn(output, target):\n","\n","    guesses = torch.argmax(output, dim=1)\n","    roots = get_roots_from_target(target)\n","    # print(\"///\")\n","    # print(\"NEW TARGET: \" + str(new_target[0]))\n","    # print(\"OLD TARGET: \" + str(target[0]))\n","    # print(\"ROOTS: \" + str(roots[0]))\n","    # print(\"GUESSESL    \" + str(guesses[0]))\n","    # print(\"OUTPUTS     \" + str(output[0]))\n","\n","    # print(guesses)\n","    # print(roots.size())\n","\n","    # print(roots)\n","    # print(guesses)\n","    # print(\"//////\")\n","\n","    evaluation = [int(guesses[i] == roots[i]) for i in range(32)]\n","\n","    return sum(evaluation)/len(evaluation)\n","\n","def train_model(model, dataloader, valloader, epochs, lr):\n","    # Loss function and optimizer\n","    criterion = torch.nn.BCEWithLogitsLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    for epoch in range(epochs):\n","        for (embeddings, targets, masks, sentences) in depth_trainloader:\n","            outputs = model(embeddings)\n","\n","            targets = only_root_target(targets.squeeze())\n","\n","            # print(targets.squeeze())\n","            # print(\"OUTPUTS: \" + str(outputs[0]))\n","            # print(\"TARGETS: \" + str(targets[0]))\n","            loss = criterion(outputs, targets.float())\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        # Print the loss for the current epoch\n","        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n","\n","if not FINAL_EVALUATION_MODE:\n","    print(\"Training depth model\")\n","\n","    # Inicjalizacja modelu\n","    depth_model = DepthModel(EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS)\n","    # TODO: ustaw hiperparametry\n","    train_model(depth_model, depth_trainloader, depth_valloader, lr=0.0001, epochs=10)\n","    # zapisz wagi modelu do pliku\n","    torch.save(depth_model.state_dict(), DEPTH_MODEL_PATH)\n","\n","\n","    #\n","    # TYMCZASOWO ZAKOMENTOWANE\n","    #\n","\n","\n","    # print(\"Training distance model\")\n","    # distance_model = DistanceModel()\n","    # # TODO: ustaw hiperparametry\n","    # train_model(distance_model, dist_trainloader, dist_valloader, lr=..., epochs=...)\n","    # # zapisz wagi modelu do pliku\n","    # torch.save(distance_model.state_dict(), DISTANCE_MODEL_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8lGDQUqEdk4r"},"outputs":[],"source":["we = get_word_embeddings([Sentence([\"Bartek\", \"wstał\", \"bardzo\", \"szybko\", \"i\", \"poszedł\", \"na\", \"zakupy\" ])],tokenizer, model)[0]\n","pad_we = my_pad_arrays((we),pad_with = 0).float()\n","\n","depth_model.eval()\n","depth_model(pad_we)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cgmQu0vbIRCy"},"outputs":[],"source":["def parse_sentence(sent: Sentence, distance_model, depth_model, tokenizer, model) -\u003e ParsedSentence:\n","    \"\"\"Zbuduj drzewo składniowe dla pojedynczego zdania.\n","\n","    Argumenty:\n","        sent: Zdanie do sparsowania.\n","        distance_model: Wytrenowany model odległości\n","        depth_model: Wytrenowany model głębokości\n","        tokenizer: Tokenizator HERBERT\n","        model: Model HERBERT\n","\n","    Zwraca:\n","        ParsedSentence: Zdanie z przewidzianym drzewem składniowym.\n","\n","    \"\"\"\n","\n","    # Twoje rozwiązanie powinno:\n","    # 1. Uzyskać embeddingi słów dla zdania.\n","    # 2. Wybrać korzeń drzewa składniowego heurystycznie, używając depth_model.\n","    # 3. Obliczyć odległości między każdą parą węzłów, używając distance_model.\n","    # 4. Zaimplementować wymyśloną przez Ciebie heurystyczną metodę wyboru krawędzi\n","    #    drzewa na podstawie przewidywanych odległości.\n","    # 5. Uzyskać obiekt ParsedSentence. Możesz użyć funkcji ParsedSentence.from_edges_and_root\n","    # 6. Zwróć obiekt ParsedSentence.\n","\n","    # Wskazówki:\n","    #  Możesz użyć sent.pretty_print() do wizualizacji sparsowanego zdania.\n","\n","    # Uwaga:\n","    # Ta funkcja zostanie użyta do oceny twojego rozwiązania. Ta funkcja powinna zwrócić rzeczywiste drzewo,\n","    # z len(sent) - 1 krawędziami. Jeśli twoje przewidywanie nie będzie drzewem, będzie ono nieprawidłowe i\n","    # nie zdobędziesz za nie punktów. Jeśli chcesz zdobyć tylko część punktów, konkurując tylko w metryce\n","    # root placement, nadal powinieneś zwrócić poprawne drzewo.\n","\n","    # TODO: implement me\n","    ...\n","\n","if not FINAL_EVALUATION_MODE:\n","    sent = train_sentences[30]\n","    parse_sentence(sent, distance_model, depth_model, tokenizer, model).pretty_print()  # Przewidziane drzewo\n","    sent.pretty_print()  # Złote drzewo (ze zbioru danych)\n","    print(sent)"]},{"cell_type":"markdown","metadata":{"id":"i_VPSmTAIRC0"},"source":["# Ewaluacja\n","Kod bardzo podobny do poniższego będzie służył do ewaluacji rozwiązania na zdaniach testowych. Wywołując poniższe komórki możesz dowiedzieć się ile punktów zdobyłoby twoje rozwiązanie, gdybyśmy ocenili je na danych walidacyjnych. Przed wysłaniem rozwiązania upewnij się, że cały notebook wykonuje się od początku do końca bez błędów i bez ingerencji użytkownika po wykonaniu polecenia `Run All`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IW89BUvxIRC0"},"outputs":[],"source":["def points(root_placement, uuas):\n","    def scale(x, lower=0.5, upper=0.85):\n","        scaled = min(max(x, lower), upper)\n","        return (scaled - lower) / (upper - lower)\n","    return (scale(root_placement) + scale(uuas))\n","\n","def evaluate_model(sentences: List[ParsedSentence], distance_model, depth_model, tokenizer, model):\n","    sum_uuas = 0\n","    root_correct = 0\n","    with torch.no_grad():\n","        for sent in sentences:\n","            parsed = parse_sentence(sent, distance_model, depth_model, tokenizer, model)\n","            root_correct += int(parsed.root == sent.root)\n","            sum_uuas += uuas_score(sent, parsed)\n","\n","    root_placement = root_correct / len(sentences)\n","    uuas = sum_uuas / len(sentences)\n","\n","    print(f\"UUAS: {uuas * 100:.3}%\")\n","    print(f\"Root placement: {root_placement * 100:.3}%\")\n","    print(f\"Your score: {points(root_placement, uuas):.1}/2.0\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WfDBN4y1IRC0"},"outputs":[],"source":["if not FINAL_EVALUATION_MODE:\n","    distance_model_loaded = DistanceModel()\n","    distance_model_loaded.load_state_dict(torch.load(DISTANCE_MODEL_PATH))\n","\n","    depth_model_loaded = DepthModel()\n","    depth_model_loaded.load_state_dict(torch.load(DEPTH_MODEL_PATH))\n","\n","    evaluate_model(val_sentences, distance_model_loaded, depth_model_loaded, tokenizer, model)"]}],"metadata":{"colab":{"collapsed_sections":["JgDgD4DTIRCg"],"name":"","version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}